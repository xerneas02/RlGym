<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ma Page</title>
    <link rel="stylesheet" href="experimentation.css"> 
</head>
<body>
    <div class="container">
        <div class="navigation-bar">
            <a href="jeu.html" class="tab">Jeu</a>
            <a href="setup.html" class="tab">Setup</a>
            <a href="architecture.html" class="tab">Architecture</a>
            <a href="#experimentation" class="tab active">Exp√©rimentation</a>
        </div>
        <div class="contenu">
            <div class="navBarre">
                <a class="navTitle" href="#constante">Constante</a> <a class="navTexte" href="#FRAME_SKIP">FRAME_SKIP</a>
                    <a class="navTexte" href="#GAME_SPEED">GAME_SPEED</a> <a class="navTexte" href="#NUM_INSTANCE">NUM_INSTANCE</a>
                    <a class="navTexte" href="#MOVE_BALL">MOVE_BALL</a> <a class="navTexte" href="#BALL_SPEED">BALL_SPEED</a>
                    <a class="navTexte" href="#AFFICHE_SCREEN">AFFICHE_SCREEN</a> <a class="navTexte" href="#SIMULATION_PER_STATS">SIMULATION_PER_STATS</a>
                    <a class="navTexte" href="#ResX & ResY">ResX & ResY</a> <a class="navTexte" href="#REPLAY_FOLDER">REPLAY_FOLDER</a>
                    <a class="navTexte" href="#LIM_X">LIM_X</a> <a class="navTexte" href="#LIM_Y">LIM_Y</a>
                    <a class="navTexte" href="#LIM_Z">LIM_Z</a> <a class="navTexte" href="#PITCH_LIM">PITCH_LIM</a>
                    <a class="navTexte" href="#YAW_LIM">YAW_LIM</a> <a class="navTexte" href="#ROLL_LIM">ROLL_LIM</a>
                    <a class="navTexte" href="#BLUE_TEAM">BLUE_TEAM</a> <a class="navTexte" href="#ORANGE_TEAM">ORANGE_TEAM</a>

                <a class="navTitle" href="#actionparser">ActionParser</a> <a class="navTexte" href="#Initialisation">Initialisation</a>
                    <a class="navTexte" href="#Espace des actions">Espace des actions</a> <a class="navTexte" href="#G√©n√©ration de la table de recherche">G√©n√©ration de la table de recherche</a>
                    <a class="navTexte" href="#Analyse des actions">Analyse des actions</a> <a class="navTexte" href="#Environnement">Environnement</a>

                <a class="navTitle" href="#Observer">Observer</a>

                <a class="navTitle" href="#Reward">Reward</a> <a class="navSousTitle" href="#Fonction de r√©compense par √©v√®nement">Fonction de r√©compense par √©v√®nement</a>
                    <a class="navTexte" href="#GoalScoredReward">GoalScoredReward</a> <a class="navTexte" href="#BoostDifferenceReward">BoostDifferenceReward</a>
                    <a class="navTexte" href="#BallTouchReward">BallTouchReward</a> <a class="navTexte" href="#DemoReward">DemoReward</a>
                    <a class="navTexte" href="#KickoffReward">KickoffReward</a> <a class="navTexte" href="#FirstTouchReward">FirstTouchReward</a>
                    <a class="navTexte" href="#SaveReward">SaveReward</a> <a class="navSousTitle" href="#Fonction de r√©compense par tick">Fonction de r√©compense par tick</a>
                    <a class="navTexte" href="#DistancePlayerBallReward">DistancePlayerBallReward</a> <a class="navTexte" href="#DistanceBallGoalReward">DistanceBallGoalReward</a>
                    <a class="navTexte" href="#FacingBallReward">FacingBallReward</a> <a class="navTexte" class="navTexte" href="#AlignBallGoalReward">AlignBallGoalReward</a>
                    <a class="navTexte" href="#ClosestToBallReward">ClosestToBallReward</a> <a class="navTexte" href="#TouchedLastReward">TouchedLastReward</a>
                    <a class="navTexte" href="#BehindBallReward">BehindBallReward</a> <a class="navTexte" href="#VelocityPlayerBallReward">VelocityPlayerBallReward</a>
                    <a class="navTexte" href="#VelocityReward">VelocityReward</a> <a class="navTexte" href="#BoostAmountReward">BoostAmountReward</a>
                    <a class="navTexte" href="#ForwardVelocityReward">ForwardVelocityReward</a> <a class="navTexte" href="#VelocityBallOwnGoalReward">VelocityBallOwnGoalReward</a>
                    <a class="navTexte" href="#VelocityBallOpponentGoalReward">VelocityBallOpponentGoalReward</a> <a class="navSousTitle" href="#P√©nalit√©s">P√©nalit√©s</a>
                    <a class="navTexte" href="#AirPenalityReward">AirPenalityReward</a> <a class="navTexte" href="#DontTouchPenalityReward">DontTouchPenalityReward</a>
                    <a class="navTexte" href="#DontGoalPenalityReward">DontGoalPenalityReward</a> <a class="navTexte" href="#BehindTheBallPenalityReward">BehindTheBallPenalityReward</a>

                <a class="navTitle" href="#state">State</a> <a class="navTexte" href="#BetterRandom">BetterRandom</a>
                    <a class="navTexte" href="#TrainingStateSetter">TrainingStateSetter</a> <a class="navTexte" href="#DefaultStateClose">DefaultStateClose</a>
                    <a class="navTexte" href="#RandomState">RandomState</a> <a class="navTexte" href="#InvertedState">InvertedState</a>
                    <a class="navTexte" href="#DefaultStateCloseOrange">DefaultStateCloseOrange</a> <a class="navTexte" href="#RandomStateOrange">RandomStateOrange</a>
                    <a class="navTexte" href="#InvertedStateOrange">InvertedStateOrange</a> <a class="navTexte" href="#LineState">LineState</a>
                    <a class="navTexte" href="#Alea">Alea</a> <a class="navTexte" href="#Attaque">Attaque</a>
                    <a class="navTexte" href="#Defense">Defense</a> <a class="navTexte" class="navTexte" href="#AirBallAD">AirBallAD</a>
                    <a class="navTexte" href="#DefenseRapide">DefenseRapide</a> <a class="navTexte" href="#Mur">Mur</a>
                    <a class="navTexte" href="#ChaosState">ChaosState</a> <a class="navTexte" href="#ReplayState">ReplayState</a>

                <a class="navTitle" href="#Callback">Callback</a>

                <a class="navTitle" href="#CustomTerminal">CustomTerminal</a>
                
                <a class="navTitle" href="#CustomPolicy">CustomPolicy</a>

                <a class="navTitle" href="#lectureGraph">lectureGraph</a>

                <a class="navTitle" href="#log_rew">log_rew</a>

                <a class="navTitle" href="#stats_bot">stats_bot</a>

               
            </div>
            <div class="menu">
                <h1>Exp√©rimentation de<br>l'apprentissage par renforcement</h1>

                <h2>Carte du terrain de rocket league</h2>

                <img src="img/coordonnee_map.png">

                <h2>Classement de bots communautaires</h2>

                <img src="./img/classe_bot.png">

                <h2>Interpr√©tation des r√©sultats du TensorBoard</h2>

                <p>
                    Le TensorBoard vous permet de r√©colter des donn√©es relatives √† votre simulation permettant ainsi d'am√©liorer les analyses et interpr√©tations des r√©sultats.
                    Voici plusieurs donn√©es ainsi que leur signification:
                </p>

                <h4>rollout/ep_len_mean</h4>
                <p>
                    C'est la longueur moyenne de chaque √©pisode d'apprentissage. Cela indique la dur√©e moyenne de chaque √©pisode d'interaction de l'agent avec l'environnement.
                </p>

                <h4>rollout/ep_rew_mean</h4>
                <p>
                    C'est la r√©compense moyenne par √©pisode. Il repr√©sente la moyenne des r√©compenses obtenues par l'agent au cours de chaque √©pisode.
                </p>

                <h4>/fps</h4>
                <p>
                    C'est le nombre de frames par seconde (images par seconde) que l'algorithme peut traiter pendant l'entra√Ænement. Cela indique la vitesse √† laquelle l'algorithme g√©n√®re des donn√©es et met √† jour les param√®tres du mod√®le.
                </p>

                <h4>time/iterations</h4>
                <p>
                    C'est le nombre d'it√©rations (ou pas de temps) que l'algorithme a effectu√©es jusqu'√† pr√©sent pendant l'entra√Ænement.
                </p>

                <h4>time/time_elapsed</h4>
                <p>
                    C'est le temps √©coul√© depuis le d√©but de l'entra√Ænement, mesur√© en secondes ou en minutes.
                </p>

                <h4>time/total_timesteps</h4>
                <p>
                    C'est le nombre total d'√©tapes (ou pas de temps) que l'algorithme a effectu√©es depuis le d√©but de l'entra√Ænement.
                </p>

                <h4>train/approx_kl</h4>
                <p>
                    Il s'agit de la divergence KL (Kullback-Leibler) approximative entre les nouvelles et anciennes politiques. Cela mesure √† quel point la nouvelle politique diff√®re de l'ancienne politique.
                </p>

                <h4>train/clip_fraction</h4>
                <p>
                    C'est la fraction d'√©chantillons qui ont √©t√© tronqu√©s lors de l'optimisation. Cela mesure √† quel point les gradients ont √©t√© tronqu√©s pour respecter la contrainte de clip lors de la mise √† jour de la politique. 
                </p>

                <h4>train/clip_range</h4>
                <p>
                    C'est la plage de clip utilis√©e pour la mise √† jour de la politique. C'est la valeur maximale de changement autoris√©e pour les param√®tres de la politique.
                </p>
                    
                <h4>train/entropy_loss</h4>
                <p>
                    C'est la perte d'entropie, qui mesure l'incertitude de la politique. Une politique plus incertaine a une entropie plus √©lev√©e.
                </p>

                <h4>train/explained_variance</h4>
                <p>
                    C'est la variance expliqu√©e par le mod√®le par rapport aux donn√©es observ√©es. Cela mesure √† quel point les pr√©dictions du mod√®le correspondent aux r√©compenses r√©elles.
                </p>

                <h4>train/learning_rate</h4>
                <p>
                    C'est le taux d'apprentissage utilis√© par l'algorithme d'optimisation pour mettre √† jour les param√®tres du mod√®le.
                </p>

                <h4>train/loss</h4>
                <p>
                    C'est la perte totale de l'algorithme. Cela mesure √† quel point les pr√©dictions du mod√®le diff√®rent des r√©compenses r√©elles.
                </p>

                <h4>train/n_updates</h4>
                <p>
                    C'est le nombre total de mises √† jour des param√®tres du mod√®le effectu√©es jusqu'√† pr√©sent pendant l'entra√Ænement.
                </p>

                <h4>train/policy_gradient_loss</h4>
                <p>
                    C'est la perte de gradient de politique, qui mesure √† quel point la politique actuelle est √©loign√©e de la politique optimale.
                </p>

                <h4>train/value_loss</h4>
                <p>
                    C'est la perte de valeur, qui mesure √† quel point les valeurs pr√©dites par le mod√®le correspondent aux valeurs r√©elles.
                </p>
                
                <h2>Resultat de la premi√®re semaine</h2>
                <p>
                    Les premiers jours du projet ont √©t√© essentiellement consacr√© √† l'installation et la mise en place des diff√©rents modules necessaires 
                    et √† la compr√©hension de l'architecture g√©n√©ral du projet. Le compte rendu de ces phases vous est pr√©sent√© dans les onglets <a href="setup.html">Setup</a>
                    et <a href="architecture.html">Architecture</a>, si vous souhaitez vous lancer dans la conception d'un bot rocket league, nous vous conseillons de vous
                    informer au pr√©alable sur ces structures et la mani√®re dont elles fonctionnent.<br><br>

                    La politique et le r√©seau de neuronne utilis√©s pendant la premi√®re semaine fut celle et celui de base propos√© par stable baselines 3.<br>
                    Il faut savoir que les r√©seaux SB3(=stable baselines 3) sont s√©par√©s en deux parties principales:<br><br>

                    Premi√®rement un extracteur de caract√©ristiques qui a pour objectif d'extraire des caract√©ristiques √† partir d'observations de grande dimension,
                    deuxi√®mement, d'un r√©seau enti√®rement connect√© mappant les fonctionnalit√©s aux actions & valeurs. Pour r√©sum√©, toutes les observations 
                    sont r√©colt√©es puis pr√©trait√©es (exemple: les obs discrets sont convertis en vecteurs uniques) avant d'√™tre 
                    transmises √† l'extracteur de caract√©ristiques. Dans le cas d'observations vectorielles, l'extracteur de caract√©ristiques n'est qu'une "Flatten" couche.<br><br>
                    
                    De plus, le terme "Politique" fait r√©f√©rence √† la classe qui g√®re tous les r√©seaux utiles √† la formation et donc pas seulement le r√©seau utilis√© pour pr√©dire les actions.<br>
                    Pour plus de pr√©cisions, je vous renvoie √† la source dont ont √©t√© extraites ces informations: <a href="https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html">Policy Networks</a>.<br><br>

                    Lors de nos tous premiers tests, nous avons pris la d√©cision d'observer la r√©action de l'agent si ce dernier n'√©tait soumis qu'√† une seule
                    et m√™me fonction de r√©compense qui se traduisait finalement par une p√©nalit√© si l'agent √©tait dans les airs.<br>
                    Apr√®s simplement 15 √† 30 minutes d'entrainement, nous nous sommes aper√ßus que l'agent ne sautait plus et conservait ses quatres roues coll√©es au sol. En
                    effet, il se contentait √† pr√©sent de simplement rouler de mani√®re al√©atoire dans le terrain.
                    Cela nous a confort√© dans l'id√©e que malgr√© une structure de r√©seau de neuronne assez basique, l'agent pouvait tout de m√™me apprendre et s'am√©liorer.<br><br>
                </p>

                <h3>Touch√© de balle</h3>
                <p>
                    Notre premier objectif fut de tenter d'apprendre √† l'agent de toucher la balle, afin de vous donner une id√©e de notre attribution de coefficient
                    selon les fonctions d√© r√©compenses:<br><br>
                </p>
                <p class="code">
                    1.45    ,  # GoalScoredReward<br>
                    0.1     ,  # BoostDifferenceReward <br>
                    1       ,  # BallTouchReward<br>
                    0.3     ,  # DemoReward<br>
                    0.0025  ,  # DistancePlayerBallReward<br>
                    0.0025  ,  # DistanceBallGoalReward<br>
                    0.000625,  # FacingBallReward<br>
                    0.0025  ,  # AlignBallGoalReward<br>
                    0.00125 ,  # ClosestToBallReward<br>
                    0.00125 ,  # TouchedLastReward<br>
                    0.00125 ,  # BehindBallReward<br>
                    0.00125 ,  # VelocityPlayerBallReward<br>
                    #0.0025 , # KickoffReward (0.1)<br>
                    #0.0025 , # KickoffReward (0.1)<br>
                    0.0025  ,  # VelocityReward (0.000625)<br>
                    0.00125 ,  # BoostAmountReward<br>
                    0.0015  ,  # ForwardVelocityReward<br>
                    3       ,  # FirstTouchReward<br>
                    #5         # AirPenalityReward<br>
                </p>
                <p>

                    Apr√®s plusieurs heures d'entrainement, nous avons pu constater une r√©action innatendu de l'agent (voir exemple ci-dessous):<br>

                    <video autoplay="autoplay" muted="" loop="" src="video/bot_doesnt_touch_ball.mp4"></video><br>

                    Nous pouvons observer que l'agent se rapproche difficilement de la balle mais sans jamais toucher cette derni√®re.
                    Apr√®s r√©flexion et l'analyse du gain de r√©compense qu'obtenait le bot pendant son entrainement, nous nous sommes
                    aper√ßut qu'il avait jug√© plus rentable de se rapprocher de la balle afin de toucher la r√©compense de "DistancePlayerBallReward",
                    plut√¥t que de directement toucher cette derni√®re.<br>
                    C'est ainsi que nous avons pris conscience de la difficult√© et de la justesse dont il faut faire preuve lorsque nous associons
                    un coefficient √† une fonction de r√©compense. C'est aussi la raison pour laquelle il faut observer attentivement l'√©volution
                    de comportement de l'agent pendant son entrainement au risque de l'entrainer en vain pendant une longue dur√©e.<br><br>

                    Par cons√©quent, nous avons opter pour un changement dans les coefficients attribu√©s aux fonctions de r√©compenses.
                    En effet, nous avons drastiquement augment√© le gain correspondant au touch√© de balle afin que cet action devienne plus rentable
                    que la distance de l'agent par rapport √† la balle.

                </p>
                <p class="code">
                    10       ,  # GoalScoredReward<br>
                    0.000000  ,  # BoostDifferenceReward <br>
                    3       ,  # BallTouchReward<br>
                    #0.3     ,  # DemoReward<br>
                    0.000000     ,  # DemoReward<br>
                    0.05    ,  # DistancePlayerBallReward<br>
                    #0.0025  ,  # DistanceBallGoalReward<br>
                    0.000000  ,  # DistanceBallGoalReward<br>
                    0.001,  # FacingBallReward<br>
                    0.0025  ,  # AlignBallGoalReward<br>
                    0.00125 ,  # ClosestToBallReward<br>
                    #0.00125 ,  # TouchedLastReward<br>
                    #0.00125 ,  # BehindBallReward<br>
                    #0.00125 ,  # VelocityPlayerBallReward<br>
                    #0.0025 ,  # KickoffReward (0.1)<br>
                    #0.05    ,  # VelocityReward (0.000625)<br>
                    #0.00125 ,  # BoostAmountReward<br>
                    #0.005  ,  # ForwardVelocityReward<br>
                    0.000000 ,  # TouchedLastReward<br>
                    0.000000 ,  # BehindBallReward<br>
                    0.000000 ,  # VelocityPlayerBallReward<br>
                    0.000000  ,  # KickoffReward (0.1)<br>
                    0.000000  ,  # VelocityReward (0.000625)<br>
                    0.000000 ,  # BoostAmountReward<br>
                    0.000000   ,  # ForwardVelocityReward<br>
                    3       ,  # FirstTouchReward<br>
                    1       ,  # DontTouchPenalityReward<br>
                    #5         # AirPenality<br>
                </p>
                <p>
                    Ensuite, apr√®s de nombreuse heure d'entrainement sur des √©tats initiaux de kickoff, l'agent √©tait en capacit√© de toucher la balle √† chaque
                    simulation. Nous avons donc pris la d√©cision d'√©tendre le nombre d'√©tats initiaux en cr√©ant la situation suivante: Les deux agents sont situ√©s
                    dans leur camp avec comme orientation le but adverse. La balle quant √† elle, est positionn√© au centre du terrain, selon l'axe X et non l'axe Y.
                    Ce qui a pour objectif d'observer les r√©actions de l'agent sur des situations autres qu'un kickoff. Nous appelerons cette situations "LineState".<br>
                </p>
                <img src="./img/lineState.png">
                <p>

                    En cons√©quence, nous avons pu remarquer que nous avions mal interpr√©t√© le r√©sultat de la simulation. Effectivement, nous 
                    pensions initialement que l'agent avait appris √† toucher la balle. En r√©alit√©, ce dernier avait juste compris qu'en avan√ßant tout droit,
                    de temps en temps, il gagnait plus de points. Etant donn√© qu'un kickoff a tendance √† favoriser le fait que la balle se trouve devant le bot,
                    il n'avait effectivement qu'√† aller tout droit de mani√®re √† percuter la balle.<br>

                    Ainsi l'entrainement n'avait servi √† rien, le bot ayant adopt√© un comportement que nous ne voulions pas. Nous avons donc r√©cup√©r√© un mod√®le de base
                    sans aucun entrainement au pr√©alable avec des coefficients similaires et pour comme seul √©tat initial, le "LineState".
                    Puis, √† la suite d'une nuit enti√®re d'entrainement, nous avons obtenu des r√©sultats beaucoup plus significatifs. En effet, l'agent r√©ussissait dans la 
                    quasi-totalit√© des cas √† toucher la balle.<br><br>

                </p>
                <img src="./img/coefLineState.png">
                <img src="./img/stateInversted.png">
                <img src="./img/result_first_semaine.png">
                <img src="./img/entropy_first_week.png">
                <img src="./img/value_loss.png">
                <p>
                    Selon les r√©sultats-ci dessus, nous pouvons observer que le temps moyen d'une simulation augmente, cela signifie si on en croit les conditions d'arr√™t
                    que le bot touche de plus en plus vite la balle. En effet, la simulation s'arr√™te en seulement 50 ticks dans le cas o√π l'agent ne toucherait pas la balle,
                    contre 2000 s'il la touche. En sachant que plus une simulation est prolong√©e, plus l'agent gagne de points car certaines r√©compenses fournissent des points √† chaque
                    tick comme la DistanceBallGoalReward qui donne plus ou moins de r√©compenses en fonction de la distance qui s√©pare l'agent de la balle. 
                    Ensuite, la r√©compense moyenne obtenu en fonction au fur et √† mesure des √©pisodes augmente elle aussi, ce qui signifie que l'agent
                    effectue de plus en plus de t√¢che qui lui rapporte des r√©compenses.<br>
                    En constate aussi une hausse de la perte d'entropie, par cons√©quent l'entropie est plus faible et cela sous-entend une politique plus certaine.
                    C'est √† dire que l'agent effectue de moins en moins d'actions al√©atoires. Cependant on remarque aussi une baisse de la variance expliqu√©e,
                    une valeur faible indique que le mod√®le ne parvient pas √† capturer la structure des donn√©es et que ses pr√©dictions ne sont pas fiables. Cela est en correlation
                    avec la hausse de la perte de valeur et un baisse de la perte du gradient.
                    Ces r√©sultats sont peu compr√©hensibles √† en constater l'obtention d'un gain plus √©lev√© de r√©compense...
                </p>
                <p>
                    Nous avons poursuivi notre √©tude en positionnant l'agent en situation r√©elle d'un match afin d'anlyser son comportement.
                    Le r√©sultat r√©siliant de ce test fut que l'agent, lorsque ce dernier se situe du bon c√¥t√© de la balle. C'est √† dire entre la balle et son propre but,
                    il n'√©prouvait que peu de difficult√© √† toucher la balle. En revanche, si l'agent se situait entre la balle et le but adverse, son comportement
                    devenait comme al√©atoire sans aucun mouvement de concret.<br>
                    Nous pouvons ainsi constater que l'agent peut √™tre r√©actif voir tr√®s r√©actif dans certaines situation qui lui ont √©t√© familiaire pendant son entrainement,
                    cependant, lorsqu'une situation lui est inconnu, les mouvements de l'agent deviennent totalement al√©atoire et son comportement perds de son sens.<br><br>

                    Afin de palier √† ce probl√®me, nous avons mis en place un nouvel √©tat initial que nous nommerons "InvertedStateOrange" ou "InvertedState", ces √©tats ont pour effet
                    de mettre l'agent dans une situation plus complexe. Effectivement, l'un le bot est toujours orient√© vers le but adverse en revanche il se situe entre
                    le but adverse et la balle. Pour l'autre, l'agent est situ√© comme √† son habitude entre la balle et son but. Cependant, il est orient√© vers son propre but. Ces deux √©tats initiaux
                    ont pour objectif de complexifier et de diversifier des situations pouvant avoir lieux pendant un match de sorte √† ce que les mouvements de l'agent, peu importe qu'il soit du bon ou du c√¥t√© de
                    la balle, ne soit plus al√©atoire.<br>
                </p>
                <img src="./img/inverted.png">
                <p>
                    A la suite de l'entrainement, nous avons de nouveau constat√© un probl√®me. Bien que le bot r√©ussissait √† pr√©sent, malgr√© son placement du mauvais c√¥t√©, √† se diriger vers la balle et √† la toucher certaines fois.
                    Ce dernier frappait la balle dans le sens inverse du jeu et parvenait m√™me √† marquer des buts contre son camp faussant ainsi l'entrainement. En effet, il faut savoir que le gain obtenu lorsque l'agent marque un but
                    est sup√©rieur √† l'ensemble des autres r√©compenses. De plus, √©tant donn√© que pour chaque entrainement le bot joue contre lui m√™me, les fonctions de r√©compenses et les donn√©es r√©colt√©es ainsi que l'analyse de ces derni√®res
                    prennent en compte aussi le bot adverse. Par cons√©quent, si un des agents marques contre son camp, il est possible que des actions effectu√©es par le bot qui n'a pas marqu√© soit valoris√©
                    alors que ces derni√®res n'auront aucun sens √©tant donn√© qu'il s'agit simplement d'un but marqu√© par le bot adverse et non pas un but marqu√© par la r√©ussite d'une suite d'actions r√©alis√©es par
                    l'agent en question.<br>
                    Cela a engendr√© la cr√©ation d
                </p>
                <h2 id="constante" class="toggle-bar" onclick="toggleMenu(1)">Constante.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content1">
                    <p>
                        Ce fichier contient l'ensemble des constantes qui sont utilis√©s par la suite dans le code,
                        nous allons vous les d√©crire une par une en sp√©cifiant leur utilit√©.
                    </p>
                    <h4 id="FRAME_SKIP" >FRAME_SKIP</h4>
                    <p>
                        Permet de d√©finir √† quelle fr√©quence on avance d'un 'time step'. Si le 'Frame skip' vaut 8, cela signifie qu'√† chaque 8 frames, on avance d'un 'time step'. Avancer d'un 'time step' implique de demander un nouvel input au bot.
                    </p>

                    <h4 id="GAME_SPEED">GAME_SPEED [0 ; 100]</h4>
                    <p>
                        Vous permet de d√©finir √† quelle vitesse vous souhaitez que votre jeu s'acc√©l√®re, la valeur 1 d√©signe la vitesse par d√©faut du jeu.
                    <p>

                    <h4 id="NUM_INSTANCE">NUM_INSTANCE [1 ; 8]</h4>
                    <p>
                        Vous permet d'ouvrir plus d'une instance de rocket league pour obtenir une simulation plus rapide (exemple: 4 ouvrira 4 fois rocket league).
                        Assurez-vous d'avoir la puissance machine requise pour ouvrir un tel nombre d'instance.
                    </p>

                    <h4 id="MOVE_BALL">MOVE_BALL [False ; True]</h4>
                    <p>
                        Il existe un grand nombre d'√©tats initiaux (state) disponible. Dans la plupart de ces derniers, la balle 
                        apparait sans vitesse initiale. Il vous suffit de passer cette constante √† True afin que la balle b√©n√©ficie
                        d'une vitesse et d'une direction al√©atoire au d√©but de la plupart des √©tats initiaux propos√©s.
                        Si vous souhaitez int√©grer cette fonctionnalit√© dans votre propre state, il vous suffira d'ajouter la ligne suivante:<br>
                        <p class="code">if MOVE_BALL : movement_ball(state_wrapper.ball)</p>
                    </p>

                    <h4 id="BALL_SPEED">BALL_SPEED [0 ; ?]</h4>
                    <p>
                        Permet de fixer une vitesse maximum √† la balle lors des √©tats initiaux √©nonc√© ci-dessus.
                    </p>

                    <h4 id="AFFICHE_SCREEN">AFFICHE_SCREEN [False ; True]</h4>
                    <p>
                        Lancer des simulations permet de r√©cup√©rer des statistiques, vous avez la possibilit√© d'observer ces statistiques en tant r√©el.
                        L'inconv√©nient est que cela engendre une ouverture et fermeture de fen√™tres en continu ce qui peut √™tre d√©rangeant si vous faites une activit√© √† c√¥t√©.
                        Si vous souhaitez tout de m√™me observer ces statistiques tout en conservant cette constante √† False, il vous suffit de taper cette
                        commande dans le terminal:<br>
                        <p class="code">python .\lectureGraph.py</p>
                    </p>

                    <h4 id="SIMULATION_PER_STATS">SIMULATION_PER_STATS [1 ; ?]</h4>
                    <p>
                        Nous avons √©voqu√© pr√©c√©dement la possibilit√© de r√©cup√©rer des statistiques pendant la simulation. Il faut savoir qu'une simulation
                        se compose de multiples sous-simulations, les statistiques sont √©tablis toutes les x sous-simulations. Cette constante vous permet de d√©finir
                        le nombre de sous-simulations minimum requises avant de r√©cup√©rer les statistiques et les stocker dans le fichier 'stats_bot.txt' .
                    </p>

                    <h4 id="ResX & ResY">ResX & ResY </h4>
                    <p>
                        Permet de d√©finir la taille des fen√™tres Rocket League pendant la simulation.<br>
                        Il est possible d'optimiser l√©g√®rement la simulation en r√©duisant la taille des fen√™tres Rocket League.
                        Il faut savoir qu'il est possible de diminuer la taille de la fen√™tre √† 0x0.
                    </p>

                    <h4 id="REPLAY_FOLDER">REPLAY_FOLDER</h4>
                    <p>
                        Chemin relatif vers le dossier contenant les donn√©es sur les replays. Ce chemin est utilis√© par le state setter <code>ReplayState</code>.
                    </p>

                    <h4 id="LIM_X">LIM_X</h4>
                    <p>
                        Coordonn√©e X maximum correspondant au mur du terrain auquel on soustrait le rayon de la balle.
                    </p>

                    <h4 id="LIM_Y">LIM_Y</h4>
                    <p>
                        Coordonn√©e Y maximum correspondant au mur du terrain auquel on soustrait le rayon de la balle.
                    </p>

                    <h4 id="LIM_Z">LIM_Z</h4>
                    <p>
                        Coordonn√©e Z maximum correspondant au plafond du terrain auquel on soustrait le rayon de la balle.
                    </p>

                    <h4 id="PITCH_LIM">PITCH_LIM</h4>
                    <p>
                        Valeur de rotation maximum sur l'axe x et y (horizontal).
                    </p>

                    <h4 id="YAW_LIM">YAW_LIM = np.pi</h4>
                    <p>
                        Valeur de rotation maximum sur l'axe z (vertical).
                    </p>

                    <h4 id="ROLL_LIM">ROLL_LIM = np.pi</h4>
                    <p>
                        Valeur de tonneau maximum
                    </p>
                    
                    <h4 id="BLUE_TEAM">BLUE_TEAM</h4>
                    <p>
                        ID de l'√©quipe Bleu    
                    </p>
                    
                    <h4 id="ORANGE_TEAM">ORANGE_TEAM</h4>
                    <p>
                        ID de l'√©quipe Orange
                    </p>
                </div>

                <h2 id="actionparser" class="toggle-bar" onclick="toggleMenu(2)">Action.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content2">
                    <p>
                        Un ActionParser est composant responsable de traduire les indices d'actions en actions concr√®tes adapt√©es √† l'environnement. Il d√©finit comment l'espace des actions est repr√©sent√© et comment les actions sont traduites des indices √† leurs valeurs correspondantes.
                    </p>

                    <h4 id="Initialisation">Initialisation</h4> 
                    <p>
                        Lors de son initialisation, ZeerLookupAction d√©finit des intervalles pr√©d√©finis ou des valeurs discr√®tes pour chaque dimension d'action. Cela signifie qu'il d√©termine √† l'avance les diff√©rentes valeurs que chaque action peut prendre, comme la vitesse, la direction, etc.
                    </p>

                    <h4 id="Espace des actions">Espace des actions</h4> 
                    <p>
                        ZeerLookupAction fournit une m√©thode pour obtenir la repr√©sentation de l'espace des actions. Dans ce cas, il s'agit d'un espace discret dont la taille est √©gale au nombre d'entr√©es dans la table de recherche.
                    </p>

                    <h4 id="G√©n√©ration de la table de recherche">G√©n√©ration de la table de recherche</h4> 
                    <p>
                        Cette impl√©mentation cr√©e une table de recherche qui associe des indices d'actions √† des valeurs d'actions concr√®tes en fonction des intervalles ou des valeurs discr√®tes d√©finies pr√©c√©demment. Cette table couvre √† la fois les actions au sol et en vol.
                    </p>

                    <h4 id="Analyse des actions">Analyse des actions</h4> 
                    <p>
                        ZeerLookupAction est capable de traduire les indices d'actions en valeurs d'actions concr√®tes en utilisant la table de recherche g√©n√©r√©e.
                    </p>

                    <h4 id="Environnement">Environnement</h4> 
                    <p>
                        Utilisation dans l'environnement : Ce composant peut √™tre utilis√© dans un environnement de simulation de Rocket League pour permettre √† un agent d'interagir avec le jeu en fournissant des actions compr√©hensibles par le moteur de jeu.
                    </p>

                </div>

                <h2 id="Observer" class="toggle-bar" onclick="toggleMenu(3)">Observer.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content3">
                    <p>
                        √† faire...
                    </p>
                </div>

                <h2 id="Reward" class="toggle-bar" onclick="toggleMenu(4)">Reward.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content4">
                    <p>
                        Ce fichier contient l'ensemble des fonctions de r√©compenses dont le r√¥le est de donner des points ou √† l'inverse des p√©nalit√©s selon le comportement
                        de l'agent. Nous allons vous pr√©sentez l'ensemble des fonctions de r√©compenses que nous avons pu utiliser jusqu'√† pr√©sent.<br>
                        Nous allons vous pr√©senter deux types de fonctions de r√©compenses, celles qui sont appel√©es lors d'un √©v√®nement pr√©cis, et celles qui sont 
                        appel√©es √† chaque tick du jeu.
                    </p>
                    <!--#######################################################################-->
                    <h3 id="Fonction de r√©compense par √©v√®nement">Fonction de r√©compense par √©v√®nement</h3>
                    <h4 id="GoalScoredReward">GoalScoredReward</h4>
                    <p>
                        L'agent est gratifi√© s'il marque un but. Dans certaines situations initiales, on donne √† la balle une vitesse et une direction al√©atoire ce 
                        qui peut conduire √† des simulations o√π la balle rentre directement dans les cages sans avoir √©t√© touch√©. Ce probl√®me est r√©gl√© avec la variable
                        globale 'TOUCH_VERIF', qui v√©rifie que la balle a √©t√© touch√© au moins une fois par un des deux agents.
                    </p>
                    <h4 id="BoostDifferenceReward">BoostDifferenceReward</h4>
                    <p>
                        L'agent est r√©compens√© lorsqu'il accumule et d√©pense du boost.
                    </p>
                    <h4 id="BallTouchReward">BallTouchReward</h4>
                    <p>
                        L'agent est r√©compens√© lorsqu'il touche la balle.
                    </p>
                    <h4 id="DemoReward">DemoReward</h4>
                    <p>
                        L'agent est r√©compens√© lorsqu'il d√©truit un adversaire (on peut d√©truire son adversaire lorsqu'on le percute avec une vitesse 'supersonic').
                    </p>
                    <h4 id="KickoffReward">KickoffReward</h4>
                    <p>
                        L'agent est r√©compens√© s'il remporte le kickoff (kickoff=engagement), si la balle est du c√¥t√© de son adversaire apr√®s le kickoff r√©alis√©, alors
                        on consid√®re ce dernier comme remport√©.
                    </p>
                    <h4 id="FirstTouchReward">FirstTouchReward</h4>
                    <p>
                        L'agent est r√©compens√© s'il touche la balle en premier lors du kickoff.
                    </p>
                    <h4 id="SaveReward">SaveReward</h4>
                    <p>
                        L'agent est r√©compens√© s'il effectue un arr√™t.
                    </p>
                    <!--######################################################################-->
                    <h3 id="Fonction de r√©compense par tick">Fonction de r√©compense par tick</h3>
                    <h4 id="DistancePlayerBallReward">DistancePlayerBallReward</h4>
                    <p>
                        Plus l'agent est proche de la balle, plus il est r√©compens√©.
                    </p>
                    <h4 id="DistanceBallGoalReward">DistanceBallGoalReward</h4>
                    <p>
                        Plus la balle est proche du but adverse (sans prendre en compte l'axe z vertical), plus l'agent est r√©compens√©.
                    </p>
                    <h4 id="FacingBallReward">FacingBallReward</h4>
                    <p>
                        Si l'agent fait face √† la balle, il est r√©compens√© (pour √©viter qu'il ne joue en arri√®re...).
                    </p>
                    <h4 id="AlignBallGoalReward">AlignBallGoalReward</h4>
                    <p>
                        Plus l'agent s'aligne par rapport √† la balle et le but adverse, plus il gagne de points.
                    </p>
                    <h4 id="ClosestToBallReward">ClosestToBallReward</h4>
                    <p>
                        Si l'agent est plus proche de la balle que son adversaire, alors il gagne des points.
                    </p>
                    <h4 id="TouchedLastReward">TouchedLastReward</h4>
                    <p>
                        Si l'agent est le dernier √† avoir touch√© la balle, alors il est r√©compens√©.
                    </p>
                    <h4 id="BehindBallReward">BehindBallReward</h4>
                    <p>
                        Si l'agent se situe derri√®re la balle, alors il est r√©compens√©. En effet, il y a peu d'int√©r√™t pour l'agent de se trouver entre la balle
                        et le but adverse.
                    </p>
                    <h4 id="VelocityPlayerBallReward">VelocityPlayerBallReward</h4>
                    <p>
                        Si l'agent se d√©place en direction de la balle, alors il est r√©compens√©.
                    </p>
                    <h4 id="VelocityReward">VelocityReward</h4>
                    <p>
                        Plus l'agent se d√©place rapidement, plus il est r√©compens√©.
                    </p>
                    <h4 id="BoostAmountReward">BoostAmountReward</h4>
                    <p>
                        Plus l'agent poss√®de du boost, plus il est r√©compens√©.
                    </p>
                    <h4 id="ForwardVelocityReward">ForwardVelocityReward</h4>
                    <p>
                        Si l'agent avance vers l'avant (p√©nalise la marche arri√®re) alors il est r√©compens√©.
                    </p>
                    <h4 id="VelocityBallOwnGoalReward">VelocityBallOwnGoalReward</h4>
                    <p>
                        a compl√©ter...
                    </p>
                    <h4 id="VelocityBallOpponentGoalReward">VelocityBallOpponentGoalReward</h4>
                    <p>
                        a compl√©ter...
                    </p>
                    <!--########################################################################################-->
                    <h3 id="P√©nalit√©s">P√©nalit√©s</h3>
                    <h4 id="AirPenalityReward">AirPenalityReward (utilisation d√©conseill√©e) </h4>
                    <p>
                        Si le joueur part dans les airs √† l'aide de son boost ou saut..etc, il est p√©nalis√©.
                    </p>
                    <h4 id="DontTouchPenalityReward">DontTouchPenalityReward</h4>
                    <p>
                        L'agent subis une p√©nalit√© croissante en fonction du temps tant qu'il n'a pas touch√© au moins une fois la balle.
                    </p>
                    <h4 id="DontGoalPenalityReward">DontGoalPenalityReward</h4>
                    <p>
                        L'agent subis une p√©nalit√© croissante en fonction du temps tant qu'il n'a pas marqu√©.
                    </p>
                    <h4 id="BehindTheBallPenalityReward">BehindTheBallPenalityReward</h4>
                    <p>
                        L'agent subis une p√©nalit√© croissante en fonction du temps tant qu'il n'est pas du bon c√¥t√© de la balle.
                    </p>
                </div>

                <h2 id="state" class="toggle-bar" onclick="toggleMenu(5)">State.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content5">
                    <p>
                        Ce fichier contient l'ensemble des fonctions proposant un √©tat initial pour une simulation. Cela permet
                        d'entrainer l'agent pour certaines situation pr√©-fabriqu√©e par nous m√™me.
                    </p>
                    <h4 id="BetterRandom">BetterRandom</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="TrainingStateSetter">TrainingStateSetter</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="DefaultStateClose">DefaultStateClose</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="RandomState">RandomState</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="InvertedState">InvertedState</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="DefaultStateCloseOrange">DefaultStateCloseOrange</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="RandomStateOrange">RandomStateOrange</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="InvertedStateOrange">InvertedStateOrange</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="LineState">LineState</h4>
                    <p>
                        Positionne la balle au centre du terrain ainsi que les agents dans leur c√¥t√© respectif. La fonction prends en param√®tre un
                        entier qui d√©termine la largeur sur l'axe Y et donc une zone dans laquelle la balle et les agents peuvent apparaitre.
                    </p>
                    <h4 id="Alea">Alea</h4>
                    <p>
                        Les voitures et le ballon spawn de mani√®re totalement al√©atoire sur le terrain avec une rotation et du boost al√©atoire.
                        Cette fonction prend deux param√®tre, le premier permet d'ajouter ou non une vitesse initiale √† la balle, le second prend
                        en consid√©ration dans cette vitesse initiale l'axe Z (vertical).
                    </p>
                    <h4 id="Attaque">Attaque</h4>
                    <p>
                        Propose une situation d'attaque & de d√©fense pour l'agent.
                    </p>
                    <h4 id="Defense">Defense</h4>
                    <p>
                        Propose une situation de d√©fense et d'attaque pour l'agent.
                    </p>
                    <h4 id="AirBallAD">AirBallAD</h4>
                    <p>
                        La balle spawn en l'air devant les cage avec un agent positionn√© pour d√©fendre et l'autre pour attaquer. L'id√©e est de travailler les balles a√©riennes.
                    </p>
                    <h4 id="DefenseRapide">DefenseRapide</h4>
                    <p>
                        Propose une situation d'attaque et de d√©fense rapide pour l'agent (la balle a une direction orient√© vers les cages).
                    </p>
                    <h4 id="Mur">Mur</h4>
                    <p>
                        La balle et l'agent spawn sur le mur latt√©ral du terrain.
                    </p>
                    <h4 id="ChaosState">ChaosState</h4>
                    <p>
                        a faire...
                    </p>
                    <h4 id="ReplayState">ReplayState</h4>
                    <p>
                        a faire...
                    </p>
                </div>

                <h2 id="Callback" class="toggle-bar" onclick="toggleMenu(6)">Callback.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content6">
                    <p>
                        a faire...
                    </p>
                </div>

                <h2 id="CustomTerminal" class="toggle-bar" onclick="toggleMenu(7)">CustomTerminal.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content7">
                    <p>
                        a faire...
                    </p>
                </div>

                <h2 id="CustomPolicy" class="toggle-bar" onclick="toggleMenu(8)">CustomPolicy.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content8">
                    <p>
                        a faire...
                    </p>
                </div>

                <h2 id="lectureGraph" class="toggle-bar" onclick="toggleMenu(9)">lectureGraph.py<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content9">
                    <p>
                        Ce fichier permet de lire les donn√©es stock√©es dans 'stats_bot.txt' et de les visualiser sous forme de graphe.
                        Penser √† utiliser la commande suivante: <br>
                        <p class="code">python .\lectureGraph.py</p>
                    </p>
                </div>

                <h2 id="log_rew" class="toggle-bar" onclick="toggleMenu(10)">log_rew.txt<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content10">
                    Ce fichier enregistre l'√©volution du gain de r√©compenses des fonctions de r√©compenses tout au long de la simulation.
                </div>

                <h2 id="stats_bot" class="toggle-bar" onclick="toggleMenu(11)">stats_bot.txt<span class="toggle-arrow">ü°á</span></h2>
                <div class="content show" id="content11">
                    <p>
                        Ce fichier contient les donn√©es actuelles (nombre de simulations, nombre de balles touch√©es, nombre de buts marqu√©s, et % de temps pass√© du bon c√¥t√© de la balle).
                        Ces donn√©es sont r√©cup√©rer toutes les 'SIMULATION_PER_STATS' simulations.
                    </p>
                </div>

            </div>    
        </div>
    </div>
    <script src="script.js"></script>
</body>
</html>